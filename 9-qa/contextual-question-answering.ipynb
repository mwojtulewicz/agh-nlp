{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Contextual question answering\n",
        "\n",
        "Mateusz Wojtulewicz"
      ],
      "metadata": {
        "id": "dVaWUR2r3eEU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup\n",
        "I'm installing useful libraries and extracting QA datasets."
      ],
      "metadata": {
        "id": "wkiZd4Vv3hz_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "id": "v2MQoq2X3mgV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bc04391-1b81-4e59-ff0a-217456270d96"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! tar -xf drive/MyDrive/studia/9semestr/nlp/poquad.tar.gz\n",
        "! unzip drive/MyDrive/studia/9semestr/nlp/simple-legal-questions-pl.zip\n",
        "\n",
        "! git clone https://github.com/huggingface/transformers.git\n",
        "\n",
        "! pip install git+https://github.com/huggingface/transformers\n",
        "! pip install git+https://github.com/huggingface/datasets\n",
        "! pip install git+https://github.com/huggingface/evaluate\n",
        "! pip install sentencepiece\n",
        "\n",
        "clear_output()\n",
        "print(\"Done.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQZgPtuKHs8g",
        "outputId": "1f705956-c230-426c-c2cf-92ec8af20fef"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab"
      ],
      "metadata": {
        "id": "EpYDeP_O3sTv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import json\n",
        "\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "9DxD6GrS3z_R"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(77)"
      ],
      "metadata": {
        "id": "RYpNx_39DCtJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data preparation\n",
        "\n",
        "I'm merging questions, relevant, passages and answers dataframes to create all-in-one dataframe for the next tasks.\n",
        "\n",
        "To create a test dataset I'm using questions for which I've provided the answer (i.e. with id in range 1093-1113)."
      ],
      "metadata": {
        "id": "LU-1aUSq32NV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "questions = pd.read_json(\"simple-legal-questions-pl/questions.jl\", lines=True).rename(\n",
        "    columns={\"_id\": \"question-id\"}\n",
        ")\n",
        "questions[\"question-id\"] = questions[\"question-id\"].astype(int)\n",
        "\n",
        "relevant = pd.read_json(\"simple-legal-questions-pl/relevant.jl\", lines=True)\n",
        "relevant[\"question-id\"] = relevant[\"question-id\"].astype(int)\n",
        "\n",
        "passages = pd.read_json(\"simple-legal-questions-pl/passages.jl\", lines=True).rename(\n",
        "    columns={\"_id\": \"passage-id\"}\n",
        ")\n",
        "\n",
        "answers = pd.read_json(\"simple-legal-questions-pl/answers.jl\", lines=True)"
      ],
      "metadata": {
        "id": "tsHErnZ134Oc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = (\n",
        "    questions\n",
        "    .merge(relevant, on=\"question-id\")\n",
        "    .merge(passages, on=\"passage-id\")\n",
        "    .merge(answers, on=\"question-id\")\n",
        "    .rename(columns={\"text_x\": \"question\", \"text_y\": \"context\"})\n",
        ")"
      ],
      "metadata": {
        "id": "1DFHAwGd43uQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 659
        },
        "id": "LZNTAfIp-nva",
        "outputId": "a9c82ca8-ebe1-4836-9663-e8ac5c77e6d8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     question-id                                           question  \\\n",
              "0             22  Co się stanie jeżeli nie zostanie uiszczona op...   \n",
              "1             23  Jak uniemożliwienić osobom nieuprawnionym dos...   \n",
              "2             24          Jaki jest tygodniowy odpoczynek kierowcy?   \n",
              "3             25  Do jakich przestępstw nie ma zastosowania częś...   \n",
              "4             26           Co robi sąd po wysłuchaniu głosów stron?   \n",
              "..           ...                                                ...   \n",
              "319         1432  Jakim przepisom podlegają przychody kościelnyc...   \n",
              "320         1433  Jakim przepisom podlegają przychody kościelnyc...   \n",
              "321         1434  Jakim przepisom podlegają przychody kościelnyc...   \n",
              "322         1435  Jakim przepisom podlegają przychody kościelnyc...   \n",
              "323         1436  Według jakiego prawa wyraz \"kosmetyki\" zastępu...   \n",
              "\n",
              "       passage-id  score_x                                              title  \\\n",
              "0    1994_195_223        1  Ustawa z dnia 30 czerwca 2000 r. Prawo własnoś...   \n",
              "1      1999_95_57        1  Ustawa z dnia 22 stycznia 1999 r. o ochronie i...   \n",
              "2     2001_1354_7        1  Ustawa z dnia 24 sierpnia 2001 r. o czasie pra...   \n",
              "3     1999_930_20        1  Ustawa z dnia 10 września 1999 r. Kodeks karny...   \n",
              "4    1997_555_408        1  Ustawa z dnia 6 czerwca 1997 r. Kodeks postępo...   \n",
              "..            ...      ...                                                ...   \n",
              "319   1995_479_29        1  Ustawa z dnia 30 czerwca 1995 r. o stosunku Pa...   \n",
              "320   1995_482_27        1  Ustawa z dnia 30 czerwca 1995 r. o stosunku Pa...   \n",
              "321   1997_554_19        1  Ustawa z dnia 13 maja 1994 r. o stosunku Państ...   \n",
              "322   1995_481_28        1  Ustawa z dnia 30 czerwca 1995 r. o stosunku Pa...   \n",
              "323   2004_623_55        1  Ustawa z dnia 4 października 2018 r. o produkt...   \n",
              "\n",
              "                                               context  score_y  \\\n",
              "0    Art. 223. 1. Opłaty jednorazowe za zgłoszenia,...      1.0   \n",
              "1    Art. 57. W celu uniemożliwienia osobom nieupra...      1.0   \n",
              "2    Art. 7. 1. W każdym tygodniu kierowca wykorzys...      1.0   \n",
              "3    Art. 20. § 1. Do przestępstw skarbowych nie ma...      1.0   \n",
              "4    Art. 408. Po wysłuchaniu głosów stron sąd niez...      1.0   \n",
              "..                                                 ...      ...   \n",
              "319  Art. 29. 1. Majątek i przychody kościelnych os...      1.0   \n",
              "320  Art. 27. 1. Majątek i przychody kościelnych os...      1.0   \n",
              "321  Art. 19. 1. Majątek i przychody kościelnych os...      1.0   \n",
              "322  Art. 28. 1. Majątek i przychody Kościoła oraz ...      1.0   \n",
              "323  Art. 55. W ustawie z dnia 19 marca 2004 r. – P...      1.0   \n",
              "\n",
              "                                                answer  \n",
              "0    postępowanie wszczęte w wyniku dokonania zgł...  \n",
              "1    należy międzyinnymi stosować wyposażenie i u...  \n",
              "2    tygodniowy odpoczynek w wymiarze co najmniej 4...  \n",
              "3                           do przestępstw skarbowych  \n",
              "4    po wysłuchaniu głosów stron sąd niezwłocznie...  \n",
              "..                                                 ...  \n",
              "319  ogólnym przepisom podatkowym, z wyjątkami okre...  \n",
              "320  ogólnym przepisom podatkowym, z wyjątkami okre...  \n",
              "321  ogólnym przepisom podatkowym, a w szczególnośc...  \n",
              "322  ogólnym przepisom podatkowym, z wyjątkami okre...  \n",
              "323  ustawa z dnia 4 października 2018 r. o produkt...  \n",
              "\n",
              "[324 rows x 8 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f32f0d3f-186f-48cd-92bd-c0af1dd6a038\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question-id</th>\n",
              "      <th>question</th>\n",
              "      <th>passage-id</th>\n",
              "      <th>score_x</th>\n",
              "      <th>title</th>\n",
              "      <th>context</th>\n",
              "      <th>score_y</th>\n",
              "      <th>answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>22</td>\n",
              "      <td>Co się stanie jeżeli nie zostanie uiszczona op...</td>\n",
              "      <td>1994_195_223</td>\n",
              "      <td>1</td>\n",
              "      <td>Ustawa z dnia 30 czerwca 2000 r. Prawo własnoś...</td>\n",
              "      <td>Art. 223. 1. Opłaty jednorazowe za zgłoszenia,...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>postępowanie wszczęte w wyniku dokonania zgł...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>23</td>\n",
              "      <td>Jak uniemożliwienić osobom nieuprawnionym dos...</td>\n",
              "      <td>1999_95_57</td>\n",
              "      <td>1</td>\n",
              "      <td>Ustawa z dnia 22 stycznia 1999 r. o ochronie i...</td>\n",
              "      <td>Art. 57. W celu uniemożliwienia osobom nieupra...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>należy międzyinnymi stosować wyposażenie i u...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>24</td>\n",
              "      <td>Jaki jest tygodniowy odpoczynek kierowcy?</td>\n",
              "      <td>2001_1354_7</td>\n",
              "      <td>1</td>\n",
              "      <td>Ustawa z dnia 24 sierpnia 2001 r. o czasie pra...</td>\n",
              "      <td>Art. 7. 1. W każdym tygodniu kierowca wykorzys...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>tygodniowy odpoczynek w wymiarze co najmniej 4...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>25</td>\n",
              "      <td>Do jakich przestępstw nie ma zastosowania częś...</td>\n",
              "      <td>1999_930_20</td>\n",
              "      <td>1</td>\n",
              "      <td>Ustawa z dnia 10 września 1999 r. Kodeks karny...</td>\n",
              "      <td>Art. 20. § 1. Do przestępstw skarbowych nie ma...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>do przestępstw skarbowych</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>26</td>\n",
              "      <td>Co robi sąd po wysłuchaniu głosów stron?</td>\n",
              "      <td>1997_555_408</td>\n",
              "      <td>1</td>\n",
              "      <td>Ustawa z dnia 6 czerwca 1997 r. Kodeks postępo...</td>\n",
              "      <td>Art. 408. Po wysłuchaniu głosów stron sąd niez...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>po wysłuchaniu głosów stron sąd niezwłocznie...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>319</th>\n",
              "      <td>1432</td>\n",
              "      <td>Jakim przepisom podlegają przychody kościelnyc...</td>\n",
              "      <td>1995_479_29</td>\n",
              "      <td>1</td>\n",
              "      <td>Ustawa z dnia 30 czerwca 1995 r. o stosunku Pa...</td>\n",
              "      <td>Art. 29. 1. Majątek i przychody kościelnych os...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>ogólnym przepisom podatkowym, z wyjątkami okre...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>320</th>\n",
              "      <td>1433</td>\n",
              "      <td>Jakim przepisom podlegają przychody kościelnyc...</td>\n",
              "      <td>1995_482_27</td>\n",
              "      <td>1</td>\n",
              "      <td>Ustawa z dnia 30 czerwca 1995 r. o stosunku Pa...</td>\n",
              "      <td>Art. 27. 1. Majątek i przychody kościelnych os...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>ogólnym przepisom podatkowym, z wyjątkami okre...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>321</th>\n",
              "      <td>1434</td>\n",
              "      <td>Jakim przepisom podlegają przychody kościelnyc...</td>\n",
              "      <td>1997_554_19</td>\n",
              "      <td>1</td>\n",
              "      <td>Ustawa z dnia 13 maja 1994 r. o stosunku Państ...</td>\n",
              "      <td>Art. 19. 1. Majątek i przychody kościelnych os...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>ogólnym przepisom podatkowym, a w szczególnośc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>322</th>\n",
              "      <td>1435</td>\n",
              "      <td>Jakim przepisom podlegają przychody kościelnyc...</td>\n",
              "      <td>1995_481_28</td>\n",
              "      <td>1</td>\n",
              "      <td>Ustawa z dnia 30 czerwca 1995 r. o stosunku Pa...</td>\n",
              "      <td>Art. 28. 1. Majątek i przychody Kościoła oraz ...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>ogólnym przepisom podatkowym, z wyjątkami okre...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>323</th>\n",
              "      <td>1436</td>\n",
              "      <td>Według jakiego prawa wyraz \"kosmetyki\" zastępu...</td>\n",
              "      <td>2004_623_55</td>\n",
              "      <td>1</td>\n",
              "      <td>Ustawa z dnia 4 października 2018 r. o produkt...</td>\n",
              "      <td>Art. 55. W ustawie z dnia 19 marca 2004 r. – P...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>ustawa z dnia 4 października 2018 r. o produkt...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>324 rows × 8 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f32f0d3f-186f-48cd-92bd-c0af1dd6a038')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f32f0d3f-186f-48cd-92bd-c0af1dd6a038 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f32f0d3f-186f-48cd-92bd-c0af1dd6a038');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before splitting into train, val and test datasets I'm dropping duplicates and leaving only questions that have an answer."
      ],
      "metadata": {
        "id": "kmtSrAjGp8ot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[df.score_y == 1]\n",
        "df = df.drop_duplicates()"
      ],
      "metadata": {
        "id": "9niCRWW8_ip4"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Splitting dataset\n",
        "\n",
        "The data subsets are prepared as follows:\n",
        "1. The `test` dataset is constructed from data examples with questions in range 1093-1113,\n",
        "2. The `val` dataset is a 20% split from the remaining dataset,\n",
        "3. The rest data samples construct the `train` dataset,\n",
        "4. Any example from the `train` dataset with a question that appears also in the `val` subset, the example is moved to the `val` dataset,\n",
        "5. Examples from `PoQuAD` dataset are added to the `train` dataset to make it have at least 1k examples."
      ],
      "metadata": {
        "id": "m7ggHXehDu88"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "questions_test = set(df.loc[df[\"question-id\"].isin(range(1093, 1114))].question.unique())"
      ],
      "metadata": {
        "id": "L4CebQU4juek"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HOZMQMVShv4",
        "outputId": "1c37c5e8-aa90-48c5-fda8-3d984093ccbf"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Czy dotychczasowe przepisy wykonawcze zachowują moc po wydaniu nowych przepisów wykonawczych?',\n",
              " 'Czy prezes Rady Ministrów określa wysokość wynagrodzenia wiceprzewodniczącego Rady Służby Cywilnej?',\n",
              " 'Czy prowadzący skład celny jest odpowiedzialny za zapewnienie, aby towary złożone w składzie celnym nie zostały usunięte?',\n",
              " 'Czy spółka partnerska jest spółką handlową?',\n",
              " 'Do czego jest proporcjonalna wielkość limitu przyznawanego producentowi suszu?',\n",
              " 'Jakiej szerokości jest pas gruntu stanowiący strefę ochronną?',\n",
              " 'Kiedy została sporządzona Międzynarodowa Konwencja Przeciwko Braniu Zakładników?',\n",
              " 'Kto dokonuje przekształcenia funduszu?',\n",
              " 'Na dołączenie czego do protokołu może zezwolić organ podatkowy?',\n",
              " 'O czym jest tekst ustawy z dnia 10 kwietnia 1974 r.?',\n",
              " 'W skład jakiego ministerstwa wchodzi Sztab Generalny Wojska Polskiego?',\n",
              " 'Z kim należy uzgadniać wysokość corocznych odpisów na fundusze specjalne NBP?',\n",
              " 'Za co odpowiada osoba przystępująca do spółki w charakterze komandytariusza?'}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.loc[:, [\"question\", \"context\", \"answer\"]]\n",
        "\n",
        "df_test = df[df.question.isin(questions_test)]\n",
        "df_trainval = df[~df.question.isin(questions_test)]\n",
        "\n",
        "df_val = df_trainval.sample(frac=0.2, random_state=77)\n",
        "\n",
        "questions_val = df_val.question.unique()\n",
        "\n",
        "df_val = df_trainval[df_trainval.question.isin(questions_val)]\n",
        "df_train = df_trainval[~df_trainval.question.isin(questions_val)]\n",
        "\n",
        "print(f\"Test dataset size : {df_test.shape[0]}\")\n",
        "print(f\"Val dataset size  : {df_val.shape[0]}\")\n",
        "print(f\"Train dataset size: {df_train.shape[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KpWUVP7FAN5O",
        "outputId": "03462ab8-7677-45ed-d415-3530789900b2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test dataset size : 14\n",
            "Val dataset size  : 72\n",
            "Train dataset size: 187\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Adding data to train subset\n",
        "\n",
        "The training subset contains only 187 samples, so I'm resizing it by adding samples from `PoQuAD` dataset."
      ],
      "metadata": {
        "id": "sdDfRAE6CpJu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"poquad/poquad_train.json\") as f:\n",
        "    poquad = json.load(f)"
      ],
      "metadata": {
        "id": "AW_mrDU2D9zM"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_poquad = {\"question\": [], \"context\": [], \"answer\": []}\n",
        "\n",
        "for topic in poquad[\"data\"]:\n",
        "    for paragraph in topic[\"paragraphs\"]:\n",
        "        for qa in paragraph[\"qas\"]:\n",
        "            if not qa[\"is_impossible\"]:\n",
        "                df_poquad[\"question\"].append(qa[\"question\"])\n",
        "                df_poquad[\"context\"].append(paragraph[\"context\"])\n",
        "                df_poquad[\"answer\"].append(qa[\"answers\"][0][\"generative_answer\"])\n",
        "\n",
        "df_poquad = pd.DataFrame(df_poquad)"
      ],
      "metadata": {
        "id": "FmFkY56FE3wJ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_poquad.shape"
      ],
      "metadata": {
        "id": "85Znw0cIENUq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa43dc37-8234-4ea9-9801-5cb16768ee46"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(30757, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = pd.concat([df_train, df_poquad.sample(n=1000)])\n",
        "\n",
        "print(f\"Train dataset size: {df_train.shape[0]}\")"
      ],
      "metadata": {
        "id": "xTMOdFZWGs4E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f011b38f-c064-4a68-9d6b-89e259147f3e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataset size: 1187\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Save data subsets in SQuAD format\n",
        "\n",
        "The subsets are saved in JSON files in SQuAD dataset format, so they can be used as inputs to `run_seq2seq_qa.py` script available in Transformers library."
      ],
      "metadata": {
        "id": "DIKayadGEgX1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def df_to_squad(df: pd.DataFrame, filename: str) -> None:\n",
        "    data = []\n",
        "    for index, row in df.iterrows():\n",
        "        data.append(\n",
        "            {\n",
        "                \"id\": str(index),\n",
        "                \"context\": row.context,\n",
        "                \"question\": row.question,\n",
        "                \"answers\": {\n",
        "                    \"text\": [row.answer],\n",
        "                    \"answer_start\": [0],\n",
        "                },\n",
        "            }\n",
        "        )\n",
        "\n",
        "    to_save = {\"data\": data}\n",
        "\n",
        "    with open(filename, \"w\", encoding=\"utf8\") as f:\n",
        "        json.dump(to_save, f, ensure_ascii=False, indent=2)"
      ],
      "metadata": {
        "id": "7OUc2c7Qdso8"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_to_squad(df=df_train, filename=\"train.json\")\n",
        "df_to_squad(df=df_val, filename=\"val.json\")\n",
        "df_to_squad(df=df_test, filename=\"test.json\")"
      ],
      "metadata": {
        "id": "TeKQMuVqevpn"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training two neural models\n",
        "\n",
        "I'm fine-tuning two pre-trained generative models able to answer the legal questions in AQA approach (`plT5-base` and `mT5-small`).\n",
        "\n",
        "For that I'm using a script available in Transformers library. The hyperparameters were set mostly to default values. The fine-tuning process is run for 10 epochs. The evaluation on the `val` subset is run at the end to compare both models' performace. Metrics that are used are `F1` and `exact_match` as those are default metrics for QA task.\n",
        "\n",
        "The fine-tuned models are saved on disk to be used later for evaluation on test questions."
      ],
      "metadata": {
        "id": "ZlQuFtu-G4vr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model: `plT5-base`"
      ],
      "metadata": {
        "id": "0HKUynUXHWCy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! python transformers/examples/pytorch/question-answering/run_seq2seq_qa.py \\\n",
        "    --model_name_or_path allegro/plt5-base \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --do_predict \\\n",
        "    --predict_with_generate \\\n",
        "    --train_file train.json \\\n",
        "    --validation_file val.json \\\n",
        "    --test_file val.json \\\n",
        "    --per_device_train_batch_size 8 \\\n",
        "    --learning_rate 3e-5 \\\n",
        "    --num_train_epochs 10 \\\n",
        "    --max_seq_length 500 \\\n",
        "    --doc_stride 128 \\\n",
        "    --output_dir plt5-base-test"
      ],
      "metadata": {
        "id": "lw2ErMzdHX2Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b535eca0-005f-4025-b9bb-9daf12789677"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "INFO:__main__:Training/evaluation parameters Seq2SeqTrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "generation_max_length=None,\n",
            "generation_num_beams=None,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=3e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=passive,\n",
            "log_level_replica=passive,\n",
            "log_on_each_node=True,\n",
            "logging_dir=plt5-base-test/runs/Jan15_19-49-55_a57cbedff694,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=10.0,\n",
            "optim=adamw_hf,\n",
            "optim_args=None,\n",
            "output_dir=plt5-base-test,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "predict_with_generate=True,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=plt5-base-test,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "sortish_sampler=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "WARNING:datasets.builder:Using custom data configuration default-7f679ce3b0dc3772\n",
            "INFO:datasets.info:Loading Dataset Infos from /usr/local/lib/python3.8/dist-packages/datasets/packaged_modules/json\n",
            "INFO:datasets.builder:Generating dataset json (/root/.cache/huggingface/datasets/json/default-7f679ce3b0dc3772/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n",
            "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-7f679ce3b0dc3772/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51...\n",
            "Downloading data files: 100% 3/3 [00:00<00:00, 8519.24it/s]\n",
            "INFO:datasets.download.download_manager:Downloading took 0.0 min\n",
            "INFO:datasets.download.download_manager:Checksum Computation took 0.0 min\n",
            "Extracting data files: 100% 3/3 [00:00<00:00, 1191.23it/s]\n",
            "INFO:datasets.utils.info_utils:Unable to verify checksums.\n",
            "INFO:datasets.builder:Generating train split\n",
            "INFO:datasets.builder:Generating validation split\n",
            "INFO:datasets.builder:Generating test split\n",
            "INFO:datasets.utils.info_utils:Unable to verify splits sizes.\n",
            "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-7f679ce3b0dc3772/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51. Subsequent calls will reuse this data.\n",
            "100% 3/3 [00:00<00:00, 686.77it/s]\n",
            "Downloading: 100% 658/658 [00:00<00:00, 464kB/s]\n",
            "[INFO|configuration_utils.py:654] 2023-01-15 19:49:59,026 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--allegro--plt5-base/snapshots/56379680948ce8b42d3d48df86569cfc210d3060/config.json\n",
            "[INFO|configuration_utils.py:706] 2023-01-15 19:49:59,030 >> Model config T5Config {\n",
            "  \"_name_or_path\": \"allegro/plt5-base\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"tokenizer_class\": \"T5Tokenizer\",\n",
            "  \"transformers_version\": \"4.26.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50048\n",
            "}\n",
            "\n",
            "Downloading: 100% 141/141 [00:00<00:00, 97.0kB/s]\n",
            "[INFO|configuration_utils.py:654] 2023-01-15 19:50:00,861 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--allegro--plt5-base/snapshots/56379680948ce8b42d3d48df86569cfc210d3060/config.json\n",
            "[INFO|configuration_utils.py:706] 2023-01-15 19:50:00,862 >> Model config T5Config {\n",
            "  \"_name_or_path\": \"allegro/plt5-base\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"tokenizer_class\": \"T5Tokenizer\",\n",
            "  \"transformers_version\": \"4.26.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50048\n",
            "}\n",
            "\n",
            "Downloading: 100% 1.12M/1.12M [00:00<00:00, 49.4MB/s]\n",
            "Downloading: 100% 65.0/65.0 [00:00<00:00, 45.3kB/s]\n",
            "[INFO|tokenization_utils_base.py:1799] 2023-01-15 19:50:05,538 >> loading file spiece.model from cache at /root/.cache/huggingface/hub/models--allegro--plt5-base/snapshots/56379680948ce8b42d3d48df86569cfc210d3060/spiece.model\n",
            "[INFO|tokenization_utils_base.py:1799] 2023-01-15 19:50:05,538 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1799] 2023-01-15 19:50:05,538 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1799] 2023-01-15 19:50:05,538 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--allegro--plt5-base/snapshots/56379680948ce8b42d3d48df86569cfc210d3060/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:1799] 2023-01-15 19:50:05,538 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--allegro--plt5-base/snapshots/56379680948ce8b42d3d48df86569cfc210d3060/tokenizer_config.json\n",
            "[INFO|configuration_utils.py:654] 2023-01-15 19:50:05,539 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--allegro--plt5-base/snapshots/56379680948ce8b42d3d48df86569cfc210d3060/config.json\n",
            "[INFO|configuration_utils.py:706] 2023-01-15 19:50:05,540 >> Model config T5Config {\n",
            "  \"_name_or_path\": \"allegro/plt5-base\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"tokenizer_class\": \"T5Tokenizer\",\n",
            "  \"transformers_version\": \"4.26.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50048\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:654] 2023-01-15 19:50:05,706 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--allegro--plt5-base/snapshots/56379680948ce8b42d3d48df86569cfc210d3060/config.json\n",
            "[INFO|configuration_utils.py:706] 2023-01-15 19:50:05,707 >> Model config T5Config {\n",
            "  \"_name_or_path\": \"allegro/plt5-base\",\n",
            "  \"architectures\": [\n",
            "    \"T5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"t5\",\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"tokenizer_class\": \"T5Tokenizer\",\n",
            "  \"transformers_version\": \"4.26.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 50048\n",
            "}\n",
            "\n",
            "Downloading: 100% 1.10G/1.10G [00:17<00:00, 62.5MB/s]\n",
            "[INFO|modeling_utils.py:2273] 2023-01-15 19:50:24,553 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--allegro--plt5-base/snapshots/56379680948ce8b42d3d48df86569cfc210d3060/pytorch_model.bin\n",
            "[INFO|configuration_utils.py:524] 2023-01-15 19:50:25,197 >> Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"transformers_version\": \"4.26.0.dev0\"\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:2855] 2023-01-15 19:50:27,962 >> All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
            "\n",
            "[INFO|modeling_utils.py:2863] 2023-01-15 19:50:27,963 >> All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at allegro/plt5-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
            "[INFO|modeling_utils.py:2520] 2023-01-15 19:50:28,885 >> Generation config file not found, using a generation config created from the model config.\n",
            "Running tokenizer on train dataset:   0% 0/2 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/json/default-7f679ce3b0dc3772/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-468fe02e104e16f1.arrow\n",
            "Running tokenizer on train dataset: 100% 2/2 [00:00<00:00,  2.34ba/s]\n",
            "Running tokenizer on validation dataset:   0% 0/1 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/json/default-7f679ce3b0dc3772/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-e2ac8115142a6edd.arrow\n",
            "Running tokenizer on validation dataset: 100% 1/1 [00:00<00:00, 17.83ba/s]\n",
            "Running tokenizer on prediction dataset:   0% 0/1 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/json/default-7f679ce3b0dc3772/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-fe99dc1e19719d31.arrow\n",
            "Running tokenizer on prediction dataset: 100% 1/1 [00:00<00:00, 17.88ba/s]\n",
            "Downloading builder script: 100% 4.53k/4.53k [00:00<00:00, 3.45MB/s]\n",
            "Downloading extra modules: 100% 3.32k/3.32k [00:00<00:00, 2.79MB/s]\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "[INFO|trainer.py:1646] 2023-01-15 19:50:39,385 >> ***** Running training *****\n",
            "[INFO|trainer.py:1647] 2023-01-15 19:50:39,385 >>   Num examples = 1187\n",
            "[INFO|trainer.py:1648] 2023-01-15 19:50:39,385 >>   Num Epochs = 10\n",
            "[INFO|trainer.py:1649] 2023-01-15 19:50:39,385 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:1650] 2023-01-15 19:50:39,385 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:1651] 2023-01-15 19:50:39,385 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1652] 2023-01-15 19:50:39,385 >>   Total optimization steps = 1490\n",
            "[INFO|trainer.py:1653] 2023-01-15 19:50:39,387 >>   Number of trainable parameters = 275102976\n",
            "  0% 0/1490 [00:00<?, ?it/s][WARNING|logging.py:281] 2023-01-15 19:50:39,412 >> You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "{'loss': 7.0351, 'learning_rate': 1.993288590604027e-05, 'epoch': 3.36}\n",
            " 34% 500/1490 [09:19<18:32,  1.12s/it][INFO|trainer.py:2705] 2023-01-15 19:59:58,635 >> Saving model checkpoint to plt5-base-test/checkpoint-500\n",
            "[INFO|configuration_utils.py:447] 2023-01-15 19:59:58,636 >> Configuration saved in plt5-base-test/checkpoint-500/config.json\n",
            "[INFO|modeling_utils.py:1702] 2023-01-15 20:00:02,527 >> Model weights saved in plt5-base-test/checkpoint-500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2157] 2023-01-15 20:00:02,528 >> tokenizer config file saved in plt5-base-test/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2164] 2023-01-15 20:00:02,528 >> Special tokens file saved in plt5-base-test/checkpoint-500/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:186] 2023-01-15 20:00:02,600 >> Copy vocab file to plt5-base-test/checkpoint-500/spiece.model\n",
            "{'loss': 3.4144, 'learning_rate': 9.865771812080538e-06, 'epoch': 6.71}\n",
            " 67% 1000/1490 [18:52<09:11,  1.13s/it][INFO|trainer.py:2705] 2023-01-15 20:09:31,698 >> Saving model checkpoint to plt5-base-test/checkpoint-1000\n",
            "[INFO|configuration_utils.py:447] 2023-01-15 20:09:31,699 >> Configuration saved in plt5-base-test/checkpoint-1000/config.json\n",
            "[INFO|modeling_utils.py:1702] 2023-01-15 20:09:35,572 >> Model weights saved in plt5-base-test/checkpoint-1000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2157] 2023-01-15 20:09:35,572 >> tokenizer config file saved in plt5-base-test/checkpoint-1000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2164] 2023-01-15 20:09:35,573 >> Special tokens file saved in plt5-base-test/checkpoint-1000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:186] 2023-01-15 20:09:35,629 >> Copy vocab file to plt5-base-test/checkpoint-1000/spiece.model\n",
            "100% 1490/1490 [28:14<00:00,  1.07it/s][INFO|trainer.py:1897] 2023-01-15 20:18:53,439 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 1694.0781, 'train_samples_per_second': 7.007, 'train_steps_per_second': 0.88, 'train_loss': 4.459088257655202, 'epoch': 10.0}\n",
            "100% 1490/1490 [28:14<00:00,  1.14s/it]\n",
            "[INFO|trainer.py:2705] 2023-01-15 20:18:53,468 >> Saving model checkpoint to plt5-base-test\n",
            "[INFO|configuration_utils.py:447] 2023-01-15 20:18:53,469 >> Configuration saved in plt5-base-test/config.json\n",
            "[INFO|modeling_utils.py:1702] 2023-01-15 20:18:57,314 >> Model weights saved in plt5-base-test/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2157] 2023-01-15 20:18:57,315 >> tokenizer config file saved in plt5-base-test/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2164] 2023-01-15 20:18:57,316 >> Special tokens file saved in plt5-base-test/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:186] 2023-01-15 20:18:57,382 >> Copy vocab file to plt5-base-test/spiece.model\n",
            "***** train metrics *****\n",
            "  epoch                    =       10.0\n",
            "  train_loss               =     4.4591\n",
            "  train_runtime            = 0:28:14.07\n",
            "  train_samples            =       1187\n",
            "  train_samples_per_second =      7.007\n",
            "  train_steps_per_second   =       0.88\n",
            "INFO:__main__:*** Evaluate ***\n",
            "[INFO|trainer.py:710] 2023-01-15 20:18:57,395 >> The following columns in the evaluation set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: offset_mapping, example_id. If offset_mapping, example_id are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2960] 2023-01-15 20:18:57,472 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2962] 2023-01-15 20:18:57,472 >>   Num examples = 74\n",
            "[INFO|trainer.py:2965] 2023-01-15 20:18:57,472 >>   Batch size = 8\n",
            "[INFO|configuration_utils.py:524] 2023-01-15 20:18:57,489 >> Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"transformers_version\": \"4.26.0.dev0\"\n",
            "}\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s][INFO|configuration_utils.py:524] 2023-01-15 20:18:58,842 >> Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"transformers_version\": \"4.26.0.dev0\"\n",
            "}\n",
            "\n",
            " 20% 2/10 [00:00<00:03,  2.53it/s][INFO|configuration_utils.py:524] 2023-01-15 20:18:59,633 >> Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"transformers_version\": \"4.26.0.dev0\"\n",
            "}\n",
            "\n",
            " 30% 3/10 [00:02<00:05,  1.30it/s][INFO|configuration_utils.py:524] 2023-01-15 20:19:00,928 >> Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"transformers_version\": \"4.26.0.dev0\"\n",
            "}\n",
            "\n",
            " 40% 4/10 [00:03<00:05,  1.14it/s][INFO|configuration_utils.py:524] 2023-01-15 20:19:01,989 >> Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"transformers_version\": \"4.26.0.dev0\"\n",
            "}\n",
            "\n",
            " 50% 5/10 [00:04<00:05,  1.04s/it][INFO|configuration_utils.py:524] 2023-01-15 20:19:03,321 >> Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"transformers_version\": \"4.26.0.dev0\"\n",
            "}\n",
            "\n",
            " 60% 6/10 [00:05<00:04,  1.10s/it][INFO|configuration_utils.py:524] 2023-01-15 20:19:04,548 >> Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"transformers_version\": \"4.26.0.dev0\"\n",
            "}\n",
            "\n",
            " 70% 7/10 [00:07<00:03,  1.18s/it][INFO|configuration_utils.py:524] 2023-01-15 20:19:05,890 >> Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"transformers_version\": \"4.26.0.dev0\"\n",
            "}\n",
            "\n",
            " 80% 8/10 [00:08<00:02,  1.23s/it][INFO|configuration_utils.py:524] 2023-01-15 20:19:07,221 >> Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"transformers_version\": \"4.26.0.dev0\"\n",
            "}\n",
            "\n",
            " 90% 9/10 [00:09<00:01,  1.24s/it][INFO|configuration_utils.py:524] 2023-01-15 20:19:08,488 >> Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"transformers_version\": \"4.26.0.dev0\"\n",
            "}\n",
            "\n",
            "100% 10/10 [00:10<00:00,  1.05s/it]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       10.0\n",
            "  eval_exact_match        =        0.0\n",
            "  eval_f1                 =      7.119\n",
            "  eval_loss               =     2.2364\n",
            "  eval_runtime            = 0:00:11.69\n",
            "  eval_samples            =         74\n",
            "  eval_samples_per_second =      6.328\n",
            "  eval_steps_per_second   =      0.855\n",
            "INFO:__main__:*** Predict ***\n",
            "[INFO|trainer.py:710] 2023-01-15 20:19:09,320 >> The following columns in the test set don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: offset_mapping, example_id. If offset_mapping, example_id are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2960] 2023-01-15 20:19:09,322 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:2962] 2023-01-15 20:19:09,322 >>   Num examples = 74\n",
            "[INFO|trainer.py:2965] 2023-01-15 20:19:09,322 >>   Batch size = 8\n",
            "[INFO|configuration_utils.py:524] 2023-01-15 20:19:09,336 >> Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"transformers_version\": \"4.26.0.dev0\"\n",
            "}\n",
            "\n",
            "  0% 0/10 [00:00<?, ?it/s][INFO|configuration_utils.py:524] 2023-01-15 20:19:10,440 >> Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"transformers_version\": \"4.26.0.dev0\"\n",
            "}\n",
            "\n",
            " 20% 2/10 [00:00<00:03,  2.54it/s][INFO|configuration_utils.py:524] 2023-01-15 20:19:11,228 >> Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"transformers_version\": \"4.26.0.dev0\"\n",
            "}\n",
            "\n",
            " 30% 3/10 [00:01<00:04,  1.46it/s][INFO|configuration_utils.py:524] 2023-01-15 20:19:12,316 >> Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"transformers_version\": \"4.26.0.dev0\"\n",
            "}\n",
            "\n",
            " 40% 4/10 [00:02<00:04,  1.22it/s][INFO|configuration_utils.py:524] 2023-01-15 20:19:13,364 >> Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"transformers_version\": \"4.26.0.dev0\"\n",
            "}\n",
            "\n",
            " 50% 5/10 [00:04<00:04,  1.08it/s][INFO|configuration_utils.py:524] 2023-01-15 20:19:14,478 >> Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"transformers_version\": \"4.26.0.dev0\"\n",
            "}\n",
            "\n",
            " 60% 6/10 [00:05<00:03,  1.01it/s][INFO|configuration_utils.py:524] 2023-01-15 20:19:15,612 >> Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"transformers_version\": \"4.26.0.dev0\"\n",
            "}\n",
            "\n",
            " 70% 7/10 [00:06<00:03,  1.03s/it][INFO|configuration_utils.py:524] 2023-01-15 20:19:16,724 >> Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"transformers_version\": \"4.26.0.dev0\"\n",
            "}\n",
            "\n",
            " 80% 8/10 [00:07<00:02,  1.07s/it][INFO|configuration_utils.py:524] 2023-01-15 20:19:17,865 >> Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"transformers_version\": \"4.26.0.dev0\"\n",
            "}\n",
            "\n",
            " 90% 9/10 [00:08<00:01,  1.09s/it][INFO|configuration_utils.py:524] 2023-01-15 20:19:18,998 >> Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"transformers_version\": \"4.26.0.dev0\"\n",
            "}\n",
            "\n",
            "100% 10/10 [00:09<00:00,  1.09it/s]***** predict metrics *****\n",
            "  predict_samples         =         74\n",
            "  test_exact_match        =        0.0\n",
            "  test_f1                 =     6.5035\n",
            "  test_loss               =     2.2364\n",
            "  test_runtime            = 0:00:10.20\n",
            "  test_samples_per_second =      7.252\n",
            "  test_steps_per_second   =       0.98\n",
            "100% 10/10 [00:09<00:00,  1.08it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model: `mT5-small`"
      ],
      "metadata": {
        "id": "EcDYZGkbKhnz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! python transformers/examples/pytorch/question-answering/run_seq2seq_qa.py \\\n",
        "    --model_name_or_path google/mt5-base \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --do_predict \\\n",
        "    --predict_with_generate \\\n",
        "    --train_file train.json \\\n",
        "    --validation_file val.json \\\n",
        "    --test_file val.json \\\n",
        "    --per_device_train_batch_size 4 \\\n",
        "    --learning_rate 3e-5 \\\n",
        "    --num_train_epochs 10 \\\n",
        "    --max_seq_length 384 \\\n",
        "    --doc_stride 128 \\\n",
        "    --output_dir mt5-base-test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VuHYeO6yjE_T",
        "outputId": "b8bba16c-8021-4b86-f68a-d640da2e0364"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:__main__:Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
            "INFO:__main__:Training/evaluation parameters Seq2SeqTrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=True,\n",
            "do_predict=True,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "generation_max_length=None,\n",
            "generation_num_beams=None,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=3e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=passive,\n",
            "log_level_replica=passive,\n",
            "log_on_each_node=True,\n",
            "logging_dir=mt5-base-test/runs/Jan15_20-21-11_a57cbedff694,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=10.0,\n",
            "optim=adamw_hf,\n",
            "optim_args=None,\n",
            "output_dir=mt5-base-test,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=4,\n",
            "predict_with_generate=True,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=mt5-base-test,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "sortish_sampler=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "WARNING:datasets.builder:Using custom data configuration default-7f679ce3b0dc3772\n",
            "INFO:datasets.info:Loading Dataset Infos from /usr/local/lib/python3.8/dist-packages/datasets/packaged_modules/json\n",
            "INFO:datasets.builder:Overwrite dataset info from restored data version.\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/json/default-7f679ce3b0dc3772/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51\n",
            "WARNING:datasets.builder:Found cached dataset json (/root/.cache/huggingface/datasets/json/default-7f679ce3b0dc3772/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51)\n",
            "INFO:datasets.info:Loading Dataset info from /root/.cache/huggingface/datasets/json/default-7f679ce3b0dc3772/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51\n",
            "100% 3/3 [00:00<00:00, 568.82it/s]\n",
            "[INFO|configuration_utils.py:654] 2023-01-15 20:21:14,705 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--mt5-base/snapshots/d86816880b5acc27e697e52bc237e816dc828b17/config.json\n",
            "[INFO|configuration_utils.py:706] 2023-01-15 20:21:14,713 >> Model config MT5Config {\n",
            "  \"_name_or_path\": \"google/mt5-base\",\n",
            "  \"architectures\": [\n",
            "    \"MT5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"mt5\",\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"tokenizer_class\": \"T5Tokenizer\",\n",
            "  \"transformers_version\": \"4.26.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 250112\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:654] 2023-01-15 20:21:15,627 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--mt5-base/snapshots/d86816880b5acc27e697e52bc237e816dc828b17/config.json\n",
            "[INFO|configuration_utils.py:706] 2023-01-15 20:21:15,628 >> Model config MT5Config {\n",
            "  \"_name_or_path\": \"google/mt5-base\",\n",
            "  \"architectures\": [\n",
            "    \"MT5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"mt5\",\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"tokenizer_class\": \"T5Tokenizer\",\n",
            "  \"transformers_version\": \"4.26.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 250112\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:1799] 2023-01-15 20:21:15,629 >> loading file spiece.model from cache at /root/.cache/huggingface/hub/models--google--mt5-base/snapshots/d86816880b5acc27e697e52bc237e816dc828b17/spiece.model\n",
            "[INFO|tokenization_utils_base.py:1799] 2023-01-15 20:21:15,629 >> loading file tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1799] 2023-01-15 20:21:15,629 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1799] 2023-01-15 20:21:15,629 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--google--mt5-base/snapshots/d86816880b5acc27e697e52bc237e816dc828b17/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:1799] 2023-01-15 20:21:15,629 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--google--mt5-base/snapshots/d86816880b5acc27e697e52bc237e816dc828b17/tokenizer_config.json\n",
            "[INFO|configuration_utils.py:654] 2023-01-15 20:21:15,629 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--mt5-base/snapshots/d86816880b5acc27e697e52bc237e816dc828b17/config.json\n",
            "[INFO|configuration_utils.py:706] 2023-01-15 20:21:15,630 >> Model config MT5Config {\n",
            "  \"_name_or_path\": \"google/mt5-base\",\n",
            "  \"architectures\": [\n",
            "    \"MT5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"mt5\",\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"tokenizer_class\": \"T5Tokenizer\",\n",
            "  \"transformers_version\": \"4.26.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 250112\n",
            "}\n",
            "\n",
            "[INFO|configuration_utils.py:654] 2023-01-15 20:21:16,049 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--mt5-base/snapshots/d86816880b5acc27e697e52bc237e816dc828b17/config.json\n",
            "[INFO|configuration_utils.py:706] 2023-01-15 20:21:16,049 >> Model config MT5Config {\n",
            "  \"_name_or_path\": \"google/mt5-base\",\n",
            "  \"architectures\": [\n",
            "    \"MT5ForConditionalGeneration\"\n",
            "  ],\n",
            "  \"d_ff\": 2048,\n",
            "  \"d_kv\": 64,\n",
            "  \"d_model\": 768,\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"dense_act_fn\": \"gelu_new\",\n",
            "  \"dropout_rate\": 0.1,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"is_gated_act\": true,\n",
            "  \"layer_norm_epsilon\": 1e-06,\n",
            "  \"model_type\": \"mt5\",\n",
            "  \"num_decoder_layers\": 12,\n",
            "  \"num_heads\": 12,\n",
            "  \"num_layers\": 12,\n",
            "  \"output_past\": true,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"relative_attention_max_distance\": 128,\n",
            "  \"relative_attention_num_buckets\": 32,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"tokenizer_class\": \"T5Tokenizer\",\n",
            "  \"transformers_version\": \"4.26.0.dev0\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 250112\n",
            "}\n",
            "\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/convert_slow_tokenizer.py:446: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n",
            "[INFO|modeling_utils.py:2273] 2023-01-15 20:21:16,718 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--google--mt5-base/snapshots/d86816880b5acc27e697e52bc237e816dc828b17/pytorch_model.bin\n",
            "[INFO|configuration_utils.py:524] 2023-01-15 20:21:18,659 >> Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"transformers_version\": \"4.26.0.dev0\"\n",
            "}\n",
            "\n",
            "[INFO|modeling_utils.py:2855] 2023-01-15 20:21:27,020 >> All model checkpoint weights were used when initializing MT5ForConditionalGeneration.\n",
            "\n",
            "[INFO|modeling_utils.py:2863] 2023-01-15 20:21:27,020 >> All the weights of MT5ForConditionalGeneration were initialized from the model checkpoint at google/mt5-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use MT5ForConditionalGeneration for predictions without further training.\n",
            "[INFO|modeling_utils.py:2520] 2023-01-15 20:21:27,944 >> Generation config file not found, using a generation config created from the model config.\n",
            "Running tokenizer on train dataset:   0% 0/2 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/json/default-7f679ce3b0dc3772/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-f87f72aef719ddae.arrow\n",
            "Running tokenizer on train dataset: 100% 2/2 [00:01<00:00,  1.95ba/s]\n",
            "Running tokenizer on validation dataset:   0% 0/1 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/json/default-7f679ce3b0dc3772/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-d9d2d286cf74e8c9.arrow\n",
            "Running tokenizer on validation dataset: 100% 1/1 [00:00<00:00,  9.11ba/s]\n",
            "Running tokenizer on prediction dataset:   0% 0/1 [00:00<?, ?ba/s]INFO:datasets.arrow_dataset:Caching processed dataset at /root/.cache/huggingface/datasets/json/default-7f679ce3b0dc3772/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51/cache-a31f227a4a0f4ce9.arrow\n",
            "Running tokenizer on prediction dataset: 100% 1/1 [00:00<00:00,  8.42ba/s]\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "[INFO|trainer.py:1646] 2023-01-15 20:21:37,564 >> ***** Running training *****\n",
            "[INFO|trainer.py:1647] 2023-01-15 20:21:37,564 >>   Num examples = 1187\n",
            "[INFO|trainer.py:1648] 2023-01-15 20:21:37,564 >>   Num Epochs = 10\n",
            "[INFO|trainer.py:1649] 2023-01-15 20:21:37,564 >>   Instantaneous batch size per device = 4\n",
            "[INFO|trainer.py:1650] 2023-01-15 20:21:37,564 >>   Total train batch size (w. parallel, distributed & accumulation) = 4\n",
            "[INFO|trainer.py:1651] 2023-01-15 20:21:37,564 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1652] 2023-01-15 20:21:37,564 >>   Total optimization steps = 2970\n",
            "[INFO|trainer.py:1653] 2023-01-15 20:21:37,565 >>   Number of trainable parameters = 582401280\n",
            "  0% 0/2970 [00:00<?, ?it/s][WARNING|logging.py:281] 2023-01-15 20:21:37,618 >> You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "{'loss': 9.1042, 'learning_rate': 2.494949494949495e-05, 'epoch': 1.68}\n",
            " 17% 500/2970 [05:47<28:17,  1.46it/s][INFO|trainer.py:2705] 2023-01-15 20:27:25,567 >> Saving model checkpoint to mt5-base-test/checkpoint-500\n",
            "[INFO|configuration_utils.py:447] 2023-01-15 20:27:25,568 >> Configuration saved in mt5-base-test/checkpoint-500/config.json\n",
            "[INFO|modeling_utils.py:1702] 2023-01-15 20:27:34,907 >> Model weights saved in mt5-base-test/checkpoint-500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2157] 2023-01-15 20:27:34,908 >> tokenizer config file saved in mt5-base-test/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2164] 2023-01-15 20:27:34,909 >> Special tokens file saved in mt5-base-test/checkpoint-500/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:186] 2023-01-15 20:27:35,246 >> Copy vocab file to mt5-base-test/checkpoint-500/spiece.model\n",
            "{'loss': 3.3817, 'learning_rate': 1.9898989898989898e-05, 'epoch': 3.37}\n",
            " 34% 1000/2970 [12:02<22:38,  1.45it/s][INFO|trainer.py:2705] 2023-01-15 20:33:40,541 >> Saving model checkpoint to mt5-base-test/checkpoint-1000\n",
            "[INFO|configuration_utils.py:447] 2023-01-15 20:33:40,542 >> Configuration saved in mt5-base-test/checkpoint-1000/config.json\n",
            "[INFO|modeling_utils.py:1702] 2023-01-15 20:33:51,132 >> Model weights saved in mt5-base-test/checkpoint-1000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2157] 2023-01-15 20:33:51,133 >> tokenizer config file saved in mt5-base-test/checkpoint-1000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2164] 2023-01-15 20:33:51,133 >> Special tokens file saved in mt5-base-test/checkpoint-1000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:186] 2023-01-15 20:33:51,730 >> Copy vocab file to mt5-base-test/checkpoint-1000/spiece.model\n",
            "{'loss': 2.4082, 'learning_rate': 1.484848484848485e-05, 'epoch': 5.05}\n",
            " 51% 1500/2970 [18:18<16:51,  1.45it/s][INFO|trainer.py:2705] 2023-01-15 20:39:56,292 >> Saving model checkpoint to mt5-base-test/checkpoint-1500\n",
            "[INFO|configuration_utils.py:447] 2023-01-15 20:39:56,293 >> Configuration saved in mt5-base-test/checkpoint-1500/config.json\n",
            "[INFO|modeling_utils.py:1702] 2023-01-15 20:40:05,630 >> Model weights saved in mt5-base-test/checkpoint-1500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2157] 2023-01-15 20:40:05,631 >> tokenizer config file saved in mt5-base-test/checkpoint-1500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2164] 2023-01-15 20:40:05,631 >> Special tokens file saved in mt5-base-test/checkpoint-1500/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:186] 2023-01-15 20:40:06,020 >> Copy vocab file to mt5-base-test/checkpoint-1500/spiece.model\n",
            "{'loss': 2.1211, 'learning_rate': 9.797979797979798e-06, 'epoch': 6.73}\n",
            " 67% 2000/2970 [24:30<11:03,  1.46it/s][INFO|trainer.py:2705] 2023-01-15 20:46:08,476 >> Saving model checkpoint to mt5-base-test/checkpoint-2000\n",
            "[INFO|configuration_utils.py:447] 2023-01-15 20:46:08,477 >> Configuration saved in mt5-base-test/checkpoint-2000/config.json\n",
            "[INFO|modeling_utils.py:1702] 2023-01-15 20:46:17,706 >> Model weights saved in mt5-base-test/checkpoint-2000/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2157] 2023-01-15 20:46:17,707 >> tokenizer config file saved in mt5-base-test/checkpoint-2000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2164] 2023-01-15 20:46:17,707 >> Special tokens file saved in mt5-base-test/checkpoint-2000/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:186] 2023-01-15 20:46:18,104 >> Copy vocab file to mt5-base-test/checkpoint-2000/spiece.model\n",
            "{'loss': 1.928, 'learning_rate': 4.747474747474747e-06, 'epoch': 8.42}\n",
            " 84% 2500/2970 [30:43<05:22,  1.46it/s][INFO|trainer.py:2705] 2023-01-15 20:52:21,593 >> Saving model checkpoint to mt5-base-test/checkpoint-2500\n",
            "[INFO|configuration_utils.py:447] 2023-01-15 20:52:21,593 >> Configuration saved in mt5-base-test/checkpoint-2500/config.json\n",
            "[INFO|modeling_utils.py:1702] 2023-01-15 20:52:30,802 >> Model weights saved in mt5-base-test/checkpoint-2500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2157] 2023-01-15 20:52:30,802 >> tokenizer config file saved in mt5-base-test/checkpoint-2500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2164] 2023-01-15 20:52:30,803 >> Special tokens file saved in mt5-base-test/checkpoint-2500/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:186] 2023-01-15 20:52:31,173 >> Copy vocab file to mt5-base-test/checkpoint-2500/spiece.model\n",
            "100% 2970/2970 [36:35<00:00,  1.52it/s][INFO|trainer.py:1897] 2023-01-15 20:58:13,371 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 2195.8907, 'train_samples_per_second': 5.406, 'train_steps_per_second': 1.353, 'train_loss': 3.4846587004484952, 'epoch': 10.0}\n",
            "100% 2970/2970 [36:35<00:00,  1.35it/s]\n",
            "[INFO|trainer.py:2705] 2023-01-15 20:58:13,459 >> Saving model checkpoint to mt5-base-test\n",
            "[INFO|configuration_utils.py:447] 2023-01-15 20:58:13,460 >> Configuration saved in mt5-base-test/config.json\n",
            "[INFO|modeling_utils.py:1702] 2023-01-15 20:58:23,058 >> Model weights saved in mt5-base-test/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:2157] 2023-01-15 20:58:23,058 >> tokenizer config file saved in mt5-base-test/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2164] 2023-01-15 20:58:23,059 >> Special tokens file saved in mt5-base-test/special_tokens_map.json\n",
            "[INFO|tokenization_t5_fast.py:186] 2023-01-15 20:58:23,450 >> Copy vocab file to mt5-base-test/spiece.model\n",
            "***** train metrics *****\n",
            "  epoch                    =       10.0\n",
            "  train_loss               =     3.4847\n",
            "  train_runtime            = 0:36:35.89\n",
            "  train_samples            =       1187\n",
            "  train_samples_per_second =      5.406\n",
            "  train_steps_per_second   =      1.353\n",
            "INFO:__main__:*** Evaluate ***\n",
            "[INFO|trainer.py:710] 2023-01-15 20:58:23,505 >> The following columns in the evaluation set don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: example_id, offset_mapping. If example_id, offset_mapping are not expected by `MT5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2960] 2023-01-15 20:58:23,510 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:2962] 2023-01-15 20:58:23,510 >>   Num examples = 83\n",
            "[INFO|trainer.py:2965] 2023-01-15 20:58:23,510 >>   Batch size = 8\n",
            "[INFO|configuration_utils.py:524] 2023-01-15 20:58:23,527 >> Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"transformers_version\": \"4.26.0.dev0\"\n",
            "}\n",
            "\n",
            "  0% 0/11 [00:00<?, ?it/s][INFO|configuration_utils.py:524] 2023-01-15 20:58:24,869 >> Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"transformers_version\": \"4.26.0.dev0\"\n",
            "}\n",
            "\n",
            " 18% 2/11 [00:00<00:02,  3.41it/s][INFO|configuration_utils.py:524] 2023-01-15 20:58:25,455 >> Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"transformers_version\": \"4.26.0.dev0\"\n",
            "}\n",
            "\n",
            " 27% 3/11 [00:01<00:03,  2.42it/s][INFO|configuration_utils.py:524] 2023-01-15 20:58:26,036 >> Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"transformers_version\": \"4.26.0.dev0\"\n",
            "}\n",
            "\n",
            " 36% 4/11 [00:02<00:05,  1.36it/s][INFO|configuration_utils.py:524] 2023-01-15 20:58:27,309 >> Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"transformers_version\": \"4.26.0.dev0\"\n",
            "}\n",
            "\n",
            " 45% 5/11 [00:03<00:04,  1.23it/s][INFO|configuration_utils.py:524] 2023-01-15 20:58:28,272 >> Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"transformers_version\": \"4.26.0.dev0\"\n",
            "}\n",
            "\n",
            " 55% 6/11 [00:04<00:03,  1.33it/s][INFO|configuration_utils.py:524] 2023-01-15 20:58:28,902 >> Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"transformers_version\": \"4.26.0.dev0\"\n",
            "}\n",
            "\n",
            " 64% 7/11 [00:04<00:02,  1.44it/s][INFO|configuration_utils.py:524] 2023-01-15 20:58:29,479 >> Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"transformers_version\": \"4.26.0.dev0\"\n",
            "}\n",
            "\n",
            " 73% 8/11 [00:05<00:01,  1.50it/s][INFO|configuration_utils.py:524] 2023-01-15 20:58:30,079 >> Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"transformers_version\": \"4.26.0.dev0\"\n",
            "}\n",
            "\n",
            " 82% 9/11 [00:05<00:01,  1.56it/s][INFO|configuration_utils.py:524] 2023-01-15 20:58:30,662 >> Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"transformers_version\": \"4.26.0.dev0\"\n",
            "}\n",
            "\n",
            " 91% 10/11 [00:06<00:00,  1.59it/s][INFO|configuration_utils.py:524] 2023-01-15 20:58:31,270 >> Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"transformers_version\": \"4.26.0.dev0\"\n",
            "}\n",
            "\n",
            "100% 11/11 [00:06<00:00,  1.62it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =       10.0\n",
            "  eval_exact_match        =     8.3333\n",
            "  eval_f1                 =    10.3412\n",
            "  eval_loss               =     1.9418\n",
            "  eval_runtime            = 0:00:08.01\n",
            "  eval_samples            =         83\n",
            "  eval_samples_per_second =     10.355\n",
            "  eval_steps_per_second   =      1.372\n",
            "INFO:__main__:*** Predict ***\n",
            "[INFO|trainer.py:710] 2023-01-15 20:58:31,662 >> The following columns in the test set don't have a corresponding argument in `MT5ForConditionalGeneration.forward` and have been ignored: example_id, offset_mapping. If example_id, offset_mapping are not expected by `MT5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2960] 2023-01-15 20:58:31,664 >> ***** Running Prediction *****\n",
            "[INFO|trainer.py:2962] 2023-01-15 20:58:31,664 >>   Num examples = 83\n",
            "[INFO|trainer.py:2965] 2023-01-15 20:58:31,664 >>   Batch size = 8\n",
            "[INFO|configuration_utils.py:524] 2023-01-15 20:58:31,678 >> Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"transformers_version\": \"4.26.0.dev0\"\n",
            "}\n",
            "\n",
            "  0% 0/11 [00:00<?, ?it/s][INFO|configuration_utils.py:524] 2023-01-15 20:58:32,741 >> Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"transformers_version\": \"4.26.0.dev0\"\n",
            "}\n",
            "\n",
            " 18% 2/11 [00:00<00:02,  3.32it/s][INFO|configuration_utils.py:524] 2023-01-15 20:58:33,343 >> Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"transformers_version\": \"4.26.0.dev0\"\n",
            "}\n",
            "\n",
            " 27% 3/11 [00:01<00:03,  2.37it/s][INFO|configuration_utils.py:524] 2023-01-15 20:58:33,936 >> Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"transformers_version\": \"4.26.0.dev0\"\n",
            "}\n",
            "\n",
            " 36% 4/11 [00:02<00:04,  1.53it/s][INFO|configuration_utils.py:524] 2023-01-15 20:58:34,983 >> Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"transformers_version\": \"4.26.0.dev0\"\n",
            "}\n",
            "\n",
            " 45% 5/11 [00:03<00:04,  1.29it/s][INFO|configuration_utils.py:524] 2023-01-15 20:58:35,981 >> Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"transformers_version\": \"4.26.0.dev0\"\n",
            "}\n",
            "\n",
            " 55% 6/11 [00:03<00:03,  1.37it/s][INFO|configuration_utils.py:524] 2023-01-15 20:58:36,625 >> Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"transformers_version\": \"4.26.0.dev0\"\n",
            "}\n",
            "\n",
            " 64% 7/11 [00:04<00:02,  1.46it/s][INFO|configuration_utils.py:524] 2023-01-15 20:58:37,211 >> Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"transformers_version\": \"4.26.0.dev0\"\n",
            "}\n",
            "\n",
            " 73% 8/11 [00:05<00:01,  1.51it/s][INFO|configuration_utils.py:524] 2023-01-15 20:58:37,826 >> Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"transformers_version\": \"4.26.0.dev0\"\n",
            "}\n",
            "\n",
            " 82% 9/11 [00:05<00:01,  1.55it/s][INFO|configuration_utils.py:524] 2023-01-15 20:58:38,428 >> Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"transformers_version\": \"4.26.0.dev0\"\n",
            "}\n",
            "\n",
            " 91% 10/11 [00:06<00:00,  1.57it/s][INFO|configuration_utils.py:524] 2023-01-15 20:58:39,046 >> Generate config GenerationConfig {\n",
            "  \"decoder_start_token_id\": 0,\n",
            "  \"eos_token_id\": 1,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"transformers_version\": \"4.26.0.dev0\"\n",
            "}\n",
            "\n",
            "100% 11/11 [00:06<00:00,  1.92it/s]***** predict metrics *****\n",
            "  predict_samples         =         83\n",
            "  test_exact_match        =     8.3333\n",
            "  test_f1                 =     9.9398\n",
            "  test_loss               =     1.9418\n",
            "  test_runtime            = 0:00:07.63\n",
            "  test_samples_per_second =     10.873\n",
            "  test_steps_per_second   =      1.441\n",
            "100% 11/11 [00:06<00:00,  1.65it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Results on `val` dataset\n",
        "\n",
        "The results on the validation dataset for both models are as follows:\n",
        "\n",
        "| Model       | f1_score [%] | exact_match [%]|\n",
        "|-------------|----------|-------------|\n",
        "| `plT5-base` | 6.50     | 0           |\n",
        "| `mT5-base`  | 9.94     | 8.33        |"
      ],
      "metadata": {
        "id": "mOVLReAOO00k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation on test questions\n",
        "\n",
        "For the `mT5-base` model that performed best on the `val` dataset after fine-tuning I'm running an evaluation loop on the `test` dataset, which consist of questions I've provided answers for.\n",
        "\n",
        "To do that I'm using transformers API to generate the answer in an autoregresive manner."
      ],
      "metadata": {
        "id": "XnCxZ7gm1OVm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "model_path = \"./mt5-base-test/\"\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_path)"
      ],
      "metadata": {
        "id": "uEYqaykNzxIi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd1fc412-bad7-4247-b775-c23989042265"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using a model of type mt5 to instantiate a model of type t5. This is not supported for all configurations of models and can yield errors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate_answer(question: str, context: str):\n",
        "    input = f\"question: {question} context: {context}\"\n",
        "\n",
        "    encoded_input = tokenizer(\n",
        "        [input], return_tensors=\"pt\", max_length=500, truncation=True\n",
        "    )\n",
        "\n",
        "    model.eval()\n",
        "    output = model.generate(\n",
        "        input_ids=encoded_input.input_ids,\n",
        "        attention_mask=encoded_input.attention_mask,\n",
        "        max_length=500,\n",
        "    )\n",
        "\n",
        "    output = tokenizer.decode(\n",
        "        output[0],\n",
        "        skip_special_tokens=True,\n",
        "    )\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "gAKn1ERO1RL0"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import string\n",
        "import re\n",
        "\n",
        "def normalize_answer(s):\n",
        "    \"\"\"Lower text and remove punctuation and extra whitespace.\"\"\"\n",
        "    def white_space_fix(text):\n",
        "        return ' '.join(text.split())\n",
        "\n",
        "    def remove_punc(text):\n",
        "        exclude = set(string.punctuation)\n",
        "        return ''.join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "    def lower(text):\n",
        "        return text.lower()\n",
        "\n",
        "    return white_space_fix(remove_punc(lower(s)))\n",
        "\n",
        "\n",
        "def f1_score(prediction, ground_truth):\n",
        "    prediction_tokens = normalize_answer(prediction).split()\n",
        "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
        "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
        "    num_same = sum(common.values())\n",
        "    if num_same == 0:\n",
        "        return 0\n",
        "    precision = 1.0 * num_same / len(prediction_tokens)\n",
        "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
        "    f1 = (2 * precision * recall) / (precision + recall)\n",
        "    return f1\n",
        "\n",
        "\n",
        "def exact_match_score(prediction, ground_truth):\n",
        "    return (normalize_answer(prediction) == normalize_answer(ground_truth))"
      ],
      "metadata": {
        "id": "LvyI92jKU43A"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = [generate_answer(r[\"question\"], r[\"context\"]) for i, r in df_test.iterrows()]"
      ],
      "metadata": {
        "id": "CvGPyWoKYgI9"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df_test.copy()\n",
        "df[\"pred\"] = predictions\n",
        "df[\"f1\"] = df.apply(lambda r: f1_score(r[\"pred\"], r[\"answer\"]), axis=1)\n",
        "df[\"exact_match\"] = df.apply(lambda r: exact_match_score(r[\"pred\"], r[\"answer\"]), axis=1)\n",
        "\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 619
        },
        "id": "tHrBkA0vaEyd",
        "outputId": "a5487b80-0641-45db-a9b7-50e0cc86c4c7"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              question  \\\n",
              "47   Jakiej szerokości jest pas gruntu stanowiący s...   \n",
              "52   Czy prezes Rady Ministrów określa wysokość wyn...   \n",
              "53              Kto dokonuje przekształcenia funduszu?   \n",
              "143  W skład jakiego ministerstwa wchodzi Sztab Gen...   \n",
              "144  W skład jakiego ministerstwa wchodzi Sztab Gen...   \n",
              "148  Z kim należy uzgadniać wysokość corocznych odp...   \n",
              "152  Na dołączenie czego do protokołu może zezwolić...   \n",
              "153  Do czego jest proporcjonalna wielkość limitu p...   \n",
              "154  Kiedy została sporządzona Międzynarodowa Konwe...   \n",
              "155        Czy spółka partnerska jest spółką handlową?   \n",
              "156  Za co odpowiada osoba przystępująca do spółki ...   \n",
              "157  Czy dotychczasowe przepisy wykonawcze zachowuj...   \n",
              "158  O czym jest tekst ustawy z dnia 10 kwietnia 19...   \n",
              "159  Czy prowadzący skład celny jest odpowiedzialny...   \n",
              "\n",
              "                                               context  \\\n",
              "47   Art. 3. 1. Wokół Pomnika Zagłady ustanawia się...   \n",
              "52   Art. 19. Prezes Rady Ministrów określa, w drod...   \n",
              "53   Art. 137. 1. Fundusz inwestycyjny zamknięty mo...   \n",
              "143  Art. 1. 1. Minister Obrony Narodowej jest nacz...   \n",
              "144  Art. 44. 1. Organami właściwymi do wyznaczania...   \n",
              "148  Art. 65. 1. Zasady tworzenia funduszu premiowe...   \n",
              "152  Art. 175. Organ podatkowy może zezwolić na doł...   \n",
              "153  Art. 60. 1. Wielkość limitu przyznanego produc...   \n",
              "154  Art. 1. Wyraża się zgodę na dokonanie przez Pr...   \n",
              "155  Art. 1. §1. Ustawa reguluje tworzenie, organiz...   \n",
              "156  Art. 114. Kto przystępuje do spółki w charakte...   \n",
              "157  Art. 6. Dotychczasowe przepisy wykonawcze zach...   \n",
              "158  Art. 7. Marszałek Sejmu, w terminie 6 miesięcy...   \n",
              "159  Art. 107. Z zastrzeżeniem art. 108, prowadzący...   \n",
              "\n",
              "                                                answer  \\\n",
              "47    nie większej niż 100 m od granic Pomnika Zagłady   \n",
              "52                                                 Tak   \n",
              "53                 towarzystwo funduszy inwestycyjnych   \n",
              "143                      Ministerstwa Obrony Narodowej   \n",
              "144                      Ministerstwa Obrony Narodowej   \n",
              "148                               z Ministrem Finansów   \n",
              "152  zeznania na piśmie podpisanego przez zeznające...   \n",
              "153  do wielkości produkowanego suszu w poprzednim ...   \n",
              "154                          w dniu 18 grudnia 1979 r.   \n",
              "155                                                Tak   \n",
              "156  za zobowiązania spółki istniejące w chwili wpi...   \n",
              "157  Tak, jeżeli nie są sprzeczne z niniejszą ustaw...   \n",
              "158         o ewidencji ludności i dowodach osobistych   \n",
              "159                                                Tak   \n",
              "\n",
              "                                                  pred        f1  exact_match  \n",
              "47                                                 tak  0.000000        False  \n",
              "52                                                 tak  1.000000         True  \n",
              "53                                                 tak  0.000000        False  \n",
              "143                                                tak  0.000000        False  \n",
              "144                                                tak  0.000000        False  \n",
              "148                                                tak  0.000000        False  \n",
              "152                                                nie  0.000000        False  \n",
              "153                                                tak  0.000000        False  \n",
              "154                                                nie  0.000000        False  \n",
              "155                                                tak  1.000000         True  \n",
              "156                                                tak  0.000000        False  \n",
              "157  nie dłużej jednak niż przez okres 12 miesięcy ...  0.777778        False  \n",
              "158                                                tak  0.000000        False  \n",
              "159                                                tak  1.000000         True  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-da7d427a-13d4-4600-b1b7-bd6f7a7c9db7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>context</th>\n",
              "      <th>answer</th>\n",
              "      <th>pred</th>\n",
              "      <th>f1</th>\n",
              "      <th>exact_match</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>Jakiej szerokości jest pas gruntu stanowiący s...</td>\n",
              "      <td>Art. 3. 1. Wokół Pomnika Zagłady ustanawia się...</td>\n",
              "      <td>nie większej niż 100 m od granic Pomnika Zagłady</td>\n",
              "      <td>tak</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>Czy prezes Rady Ministrów określa wysokość wyn...</td>\n",
              "      <td>Art. 19. Prezes Rady Ministrów określa, w drod...</td>\n",
              "      <td>Tak</td>\n",
              "      <td>tak</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>Kto dokonuje przekształcenia funduszu?</td>\n",
              "      <td>Art. 137. 1. Fundusz inwestycyjny zamknięty mo...</td>\n",
              "      <td>towarzystwo funduszy inwestycyjnych</td>\n",
              "      <td>tak</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>143</th>\n",
              "      <td>W skład jakiego ministerstwa wchodzi Sztab Gen...</td>\n",
              "      <td>Art. 1. 1. Minister Obrony Narodowej jest nacz...</td>\n",
              "      <td>Ministerstwa Obrony Narodowej</td>\n",
              "      <td>tak</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>144</th>\n",
              "      <td>W skład jakiego ministerstwa wchodzi Sztab Gen...</td>\n",
              "      <td>Art. 44. 1. Organami właściwymi do wyznaczania...</td>\n",
              "      <td>Ministerstwa Obrony Narodowej</td>\n",
              "      <td>tak</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148</th>\n",
              "      <td>Z kim należy uzgadniać wysokość corocznych odp...</td>\n",
              "      <td>Art. 65. 1. Zasady tworzenia funduszu premiowe...</td>\n",
              "      <td>z Ministrem Finansów</td>\n",
              "      <td>tak</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>152</th>\n",
              "      <td>Na dołączenie czego do protokołu może zezwolić...</td>\n",
              "      <td>Art. 175. Organ podatkowy może zezwolić na doł...</td>\n",
              "      <td>zeznania na piśmie podpisanego przez zeznające...</td>\n",
              "      <td>nie</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>153</th>\n",
              "      <td>Do czego jest proporcjonalna wielkość limitu p...</td>\n",
              "      <td>Art. 60. 1. Wielkość limitu przyznanego produc...</td>\n",
              "      <td>do wielkości produkowanego suszu w poprzednim ...</td>\n",
              "      <td>tak</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>154</th>\n",
              "      <td>Kiedy została sporządzona Międzynarodowa Konwe...</td>\n",
              "      <td>Art. 1. Wyraża się zgodę na dokonanie przez Pr...</td>\n",
              "      <td>w dniu 18 grudnia 1979 r.</td>\n",
              "      <td>nie</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>155</th>\n",
              "      <td>Czy spółka partnerska jest spółką handlową?</td>\n",
              "      <td>Art. 1. §1. Ustawa reguluje tworzenie, organiz...</td>\n",
              "      <td>Tak</td>\n",
              "      <td>tak</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>156</th>\n",
              "      <td>Za co odpowiada osoba przystępująca do spółki ...</td>\n",
              "      <td>Art. 114. Kto przystępuje do spółki w charakte...</td>\n",
              "      <td>za zobowiązania spółki istniejące w chwili wpi...</td>\n",
              "      <td>tak</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>157</th>\n",
              "      <td>Czy dotychczasowe przepisy wykonawcze zachowuj...</td>\n",
              "      <td>Art. 6. Dotychczasowe przepisy wykonawcze zach...</td>\n",
              "      <td>Tak, jeżeli nie są sprzeczne z niniejszą ustaw...</td>\n",
              "      <td>nie dłużej jednak niż przez okres 12 miesięcy ...</td>\n",
              "      <td>0.777778</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>158</th>\n",
              "      <td>O czym jest tekst ustawy z dnia 10 kwietnia 19...</td>\n",
              "      <td>Art. 7. Marszałek Sejmu, w terminie 6 miesięcy...</td>\n",
              "      <td>o ewidencji ludności i dowodach osobistych</td>\n",
              "      <td>tak</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>159</th>\n",
              "      <td>Czy prowadzący skład celny jest odpowiedzialny...</td>\n",
              "      <td>Art. 107. Z zastrzeżeniem art. 108, prowadzący...</td>\n",
              "      <td>Tak</td>\n",
              "      <td>tak</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-da7d427a-13d4-4600-b1b7-bd6f7a7c9db7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-da7d427a-13d4-4600-b1b7-bd6f7a7c9db7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-da7d427a-13d4-4600-b1b7-bd6f7a7c9db7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f1 = df.f1.mean() * 100\n",
        "exact_match = df.exact_match.mean() * 100\n",
        "\n",
        "print(\"Results on test dataset:\")\n",
        "print(f\"F1 score         : {f1:.3f}\")\n",
        "print(f\"exact_match score: {exact_match:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ak_6RyjzQbUj",
        "outputId": "45be23a1-768b-48a1-87b1-53a15dbc0b57"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results on test dataset:\n",
            "F1 score         : 26.984\n",
            "exact_match score: 21.429\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Results on `test` dataset\n",
        "\n",
        "The following table shows the results of `mT5-base` model on both `val` and `test` datasets:\n",
        "\n",
        "| dataset | f1_score [%] | exact_match [%] |\n",
        "|---------|----------|-------------|\n",
        "| `val`   | 9.94     | 8.33        |\n",
        "| `test`  | 26.98    | 21.43       |\n",
        "\n"
      ],
      "metadata": {
        "id": "xi6gSDiARLNN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Answering questions\n",
        "\n",
        "#### 1. Which pre-trained model performs better on that task?\n",
        "\n",
        "The multilingual `mT5-base` model,  compared to polish-only `plT5-base` model, performed better on that task, acquiring higher F1 score (9.94 % vs 6.50 %) and exact match percentage (8.33 % vs 0 %).\n",
        "\n",
        "However, the training was longer for the `mT5-base` model, mostly because it is bigger compared to `plT5-base` model, and for the training to fit on the GPU I had to lower the batch size from 8 samples to 4.\n",
        "\n",
        "\n",
        "#### 2. Does the performance on the validation dataset reflects the performance on your test set?\n",
        "\n",
        "Suprisingly that was not the case, because the model performed much better on `test` dataset than on `val` dataset. This is because the `test` dataset consists of easier questions, with short `Tak` answer, which boosted both F1 score and exact match score if the model get it right, which it mostly did because of some strange yes/no bias.\n",
        "\n",
        "\n",
        "#### 3. What are the outcomes of the model on your own questions? Are they satisfying? If not, what might be the reason for that?\n",
        "\n",
        "The outcomes on my own questions are not satisfactory. The model appears to have a bias towards answering shortly, with yes or no. For my questions it happened to be pretty good though. \n",
        "\n",
        "\n",
        "\n",
        "#### 4. Why extractive question answering is not well suited for inflectional languages?\n",
        "\n",
        "For question answering task one has to understand the context, as well as the answer. In inflectional languages the words can take not only various forms, depending on the usage, but also arbitral order (e.g. _Jan zjadł rybę_, and _Rybę zjadł Jan_ has the same meaning but different order). Those facts make it harder to understand the meaning of the sentence and extract useful information for answering the question.\n",
        "\n",
        "\n",
        "#### 5. Why you have to remove the duplicated questions from the training and the validation subsets?\n",
        "\n",
        "\n",
        "The validation and test dataset has to contain examples that do not appear in train dataset, otherwise the evaluation won't be reliable. In QA task the example is not only understood as the question, context, answer tuple, but also as one question but different context, because the information that is beeing seeked is the same."
      ],
      "metadata": {
        "id": "5NF3QSDnSDaB"
      }
    }
  ]
}